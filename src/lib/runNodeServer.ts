/*
 * Enhanced runFlow():
 *   â€“ keeps a ctx.seen array (topics already covered)
 *   â€“ recognises `__resume__` sentinel to pick the next pending story node
 *   â€“ skips any story node whose tag already appears in ctx.seen
 */
import OpenAI from "openai";
import { FlowEngine } from "@/lib/flowEngine";
import type { ChatNode } from "@/lib/flow";
import { flow } from "@/lib/flow";

const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY! });

export async function answerUnknown(txt:string){
  const r = await openai.chat.completions.create({
    model:"gpt-3.5-turbo",
    temperature:0.7,
    messages:[
      {role:"system",content:"Ğ¢Ñ‹ Ğ´Ñ€ÑƒĞ¶ĞµĞ»ÑĞ±Ğ½Ñ‹Ğ¹ ÑĞ²Ğ°Ğ´ĞµĞ±Ğ½Ñ‹Ğ¹ Ğ±Ğ¾Ñ‚-ĞºĞ¾Ğ½ÑÑŒĞµÑ€Ğ¶. Ğ˜Ğ·Ğ²Ğ¸Ğ½Ğ¸ÑÑŒ Ğ¸ ÑĞºĞ°Ğ¶Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ Ğ´Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ñ‚ĞµĞ¼Ğµ Ğ½ĞµÑ‚."},
      {role:"user",content:txt}
    ],
  });
  return r.choices[0].message?.content ?? "Ğ˜Ğ·Ğ²Ğ¸Ğ½Ğ¸Ñ‚Ğµ, Ñƒ Ğ¼ĞµĞ½Ñ Ğ½ĞµÑ‚ Ñ‚Ğ°ĞºĞ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ ğŸ™ˆ";
}

/* â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ helper â”€â”€ */
function fill(raw: string, ctx: any, userText: string) {
  // if the template is literally "{intro}" and ctx.intro exists â†’ use it
  let txt = raw === "{intro}" && ctx.intro ? ctx.intro : raw;

  if (raw === "{intro}" && !ctx.intro) {
    txt = `ĞŸÑ€Ğ¸Ğ²ĞµÑ‚, ${ctx.name}!`;
  }

  // simple placeholders
  return txt
    .replace(/<name>/g,  ctx.name ?? "")
    .replace(/<diet>/g,  ctx.diet ?? "")
    .replace(/<stays>/g, ctx.stays ? "Ğ¾ÑÑ‚Ğ°ĞµÑ‚ÑÑ Ñ Ğ½Ğ¾Ñ‡Ñ‘Ğ²ĞºĞ¾Ğ¹" : "")
    .replace(/<input>/g, userText);           // if you ever need last answer
}

export async function renderNode(node: ChatNode, ctx: any, userText: string) {
  const raw = Array.isArray(node.template)
    ? node.template[Math.floor(Math.random() * node.template.length)]
    : node.template;
  const compiled = fill(raw, ctx, userText);

  if (node.useGPT === false) {
    const out: any[] = [{ role: "bot", type: "text", text: compiled }];
    if (node.info) out.push({ role: "bot", type: "info", ...node.info });
    return out;
  }

  /* GPT as a â€˜glueâ€™ layer: forward userText, but keep story prompt locked */
  const r = await openai.chat.completions.create({
    model: "gpt-4o-mini",
    temperature: 0.7,
    messages: [
      { role: "system", content: compiled },
      { role: "user", content: userText },
    ],
  });
  return [{ role: "bot", type: "text", text: r.choices[0].message?.content ?? "" }];
}

/**
 * Core runner: single requestâ€“response cycle.
 * Handles `ctx.seen` + â€œresumeâ€ sentinel.
 */
export async function runFlow(stateId: string, input: string, ctx: any) {
  // ensure ctx.seen exists
  if (!Array.isArray(ctx.seen)) ctx.seen = [] as string[];
  if (!Array.isArray(ctx.states)) ctx.states = [] as any[];

  /* â”€â”€ 0. init engine â”€â”€ */
  const engine = new FlowEngine(stateId);
  let userInput = input.trim();

  /* â”€â”€ 1. advance immediately if user typed Ñ‡Ñ‚Ğ¾â€‘Ñ‚Ğ¾ â”€â”€ */
  if (userInput) {
    const currentNode = engine.node();
    const expectsButton = currentNode.buttons?.length;
    const expectsFree   = !currentNode.auto && !expectsButton; // ÑƒĞ·ĞµĞ» Ğ¶Ğ´ĞµÑ‚ ÑĞ²Ğ¾Ğ±Ğ¾Ğ´Ğ½Ñ‹Ğ¹ Ğ²Ğ²Ğ¾Ğ´

    if (expectsButton || expectsFree) {
      engine.advance(userInput, ctx);
      userInput = ""; // consume once
    }

    //if (currentNode.tag) ctx.seen.push(currentNode.tag);
  }

  /* â”€â”€ 2. build bot messages until Ğ¾ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ğ¼ÑÑ â”€â”€ */
  const messages: any[] = [];
  for (let guard = 0; guard < 30; guard++) {
    const node = engine.node();
    if (!node) break; // corrupted id

    /* skip duplicate topics */
    if (node.tag && ctx.seen.includes(node.tag)) {
      if (!node.inquiry && node.tag)
        messages.push(...(await renderNode(node, ctx, userInput)));
      engine.advance("", ctx);
      continue;
    }

    /* render */
    messages.push(...(await renderNode(node, ctx, userInput)));
    if (node.delayMs)
      messages.push({ role: "bot", type: "typing", delayMs: node.delayMs });

    /* if node is auto â†’ jump Ğ´Ğ°Ğ»ÑŒÑˆĞµ Ğ¸ Ğ¿Ñ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶Ğ¸Ñ‚ÑŒ Ñ†Ğ¸ĞºĞ» */
    if (node.auto) {
      engine.advance("", ctx);
      continue;
    }

    /* interactive node: stop here */
    return {
      state: engine.id(),
      buttons: node.buttons ?? [],
      messages,
    };
  }

  /* â”€â”€ safety fallback â”€â”€ */
  return {
    state: engine.id(),
    buttons: [],
    messages,
  };
}